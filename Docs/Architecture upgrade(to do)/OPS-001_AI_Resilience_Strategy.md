OPS-001: AI Layer Resilience StrategyMeta FieldValueStatusDRAFTDate2025-12-17AuthorSenior ArchitectScopeapps/worker, apps/apiPatternsCircuit Breaker, Bulkhead, Graceful Degradation, Rate Limiting1. The Core Philosophy"The goal is not to never fail. The goal is to fail without hurting the user." [Ref: 01-stability-and-resilience/README.md]Integrating Probabilistic AI (LLMs) into a Deterministic OS (Orgo) introduces three new risks:Latency Spikes: LLMs can take 30+ seconds to respond.Non-Determinism: The same input can yield different outputs (or errors).Cost Explosions: Infinite loops in AI agents can drain the budget in minutes.This document defines the Defensive Architecture required to mitigate these risks.2. Protection Layer 1: Circuit BreakersPattern Reference:We wrap all external AI calls (SenTient API, Architect LLM) in a proxy that monitors for failures. If the provider is unstable, we "trip" the circuit to protect our internal thread pools.2.1 Configuration SpecWe use Opossum (Node.js) or similar libraries with these strict settings:SettingValueRationaleTimeout8000msBackground jobs must tick. Waiting 30s for an email generation is unacceptable.Error Threshold50%If half of requests fail, the provider is effectively down.Volume Threshold10 reqsDon't trip on a single failure; wait for a pattern.Reset Timeout30,000msGive the provider 30s to recover before letting traffic through again.2.2 Implementation LogicTypeScript// apps/api/src/adapters/ai-circuit-breaker.ts
const breaker = new CircuitBreaker(callOpenAI, {
  timeout: 8000,
  errorThresholdPercentage: 50,
  resetTimeout: 30000
});

breaker.fallback(() => {
  // FAST FAIL: Do not retry. Do not wait.
  // Return a special symbol indicating the AI is offline.
  return { status: 'FALLBACK_MODE', content: null };
});

breaker.on('open', () => logger.warn('⚠️ AI Circuit OPEN: Switching to Manual/Static modes'));
breaker.on('close', () => logger.info('✅ AI Circuit CLOSED: Resuming Intelligence'));
3. Protection Layer 2: Bulkhead PatternPattern Reference:We must isolate the "Intelligence" workload from the "Operational" workload. If 1,000 users trigger AI summaries simultaneously, they must not starve the CPU resources needed for Login or Database writes.3.1 Queue Isolation (Redis/BullMQ)We define strict "Lanes" for processing.Queue NamePriorityConcurrencyPurposeQueue:SystemHigh50Emails, Password Resets, Critical Alerts.Queue:IngestionMedium20SenTient email parsing (I/O Bound).Queue:ExpressionLow5Architect LLM Generation (Rate Limit Bound).3.2 The "Titanic" StrategyIf Queue:Expression fills up (10,000 pending notifications):The System and Ingestion queues continue running unaffected.The Notification system slows down, but the Core OS remains snappy.Load Shedding: If the queue age exceeds 1 hour, we drop the "Generate Text" job and fallback to "Send Template" immediately.4. Protection Layer 3: Graceful DegradationPattern Reference:When the Circuit is OPEN or the Bulkhead is FULL, we do not show an error screen. We degrade the experience.4.1 Input Degradation (SenTient)Normal: AI parses email -> Creates structured Task.Degraded: System accepts email -> Creates generic "Uncategorized Task" with raw body -> Flags for "Manual Review".User Experience: The user sees "Task Created" (Success), but an internal admin sees "Needs Classification."4.2 Output Degradation (Architect)Normal: AI writes a custom, empathetic email to the user.Degraded: System pulls a pre-written Handlebars template (templates/fallback-notification.hbs).User Experience: User receives a standard "You have a new notification" email. It is boring, but functional.5. Protection Layer 4: Rate Limiting (Cost Control)Pattern Reference:LLMs charge per token. An infinite loop or a malicious user could drain our API credits.5.1 Token Bucket StrategyWe implement a Global Token Budget in Redis.Limit: $50.00 USD / Day.Check: Before calling Architect, check GET org:daily_spend.Action:If spend < limit: Proceed.If spend >= limit: Fail Fast. Log "Budget Exceeded" and use Fallback Templates.5.2 Per-Tenant LimitsFree Tier: 10 AI generations / day.Pro Tier: 1,000 AI generations / day.Implementation: RateLimitService middleware on the OutboxPoller.6. Observability & Golden SignalsPattern Reference:We cannot fix what we cannot see. We track specific metrics for the AI layer.6.1 The Dashboard (Grafana)MetricThreshold (Alert)Meaningai_circuit_open> 0The AI provider is down. System is in Fallback.sentient_parse_failure> 5%The input model is drifting or broken.architect_latency_p99> 10sThe LLM is stalling; user experience is degrading.dlq_depth> 0Poison pills detected. Admin action required.6.2 Structured LoggingEvery AI interaction must be logged with a correlation_id and model_version.JSON{
  "level": "info",
  "event": "ai_generation_complete",
  "correlation_id": "task_123_abc",
  "model": "gpt-4-turbo",
  "tokens_input": 500,
  "tokens_output": 120,
  "cost_usd": 0.004,
  "duration_ms": 4500
}
7. Incident PlaybooksScenario A: "The Hallucination Storm"Symptom: Users report that Architect is sending offensive or nonsensical emails.Action:Kill Switch: Run npm run admin:kill-switch:architect (Sets Feature Flag use_ai_generation to FALSE).System State: Instantly reverts to Static Templates.Investigate: Check NotificationLog table to find the specific prompt causing issues.Scenario B: "The Queue Backlog"Symptom: Queue:Expression has 50,000 jobs. Notifications are delayed by 2 hours.Action:Load Shed: Run script to flush pending jobs older than 10 minutes.Fallback: The flushed jobs will fail. The OutboxPoller creates a standard email for them on retry (because the outbox.retryCount will exceed the AI threshold).Scale: Temporarily increase concurrency to 20 (if API limits allow).